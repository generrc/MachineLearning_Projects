{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW \\#1: Deep Q Networks: VFA for Q-Learning\n",
    "\n",
    "**Name:**  <font color=\"red\">Alexander Johnston</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Overview\n",
    "\n",
    "## Objective/Approach\n",
    "The objective of this assignment is to train a Reinforcement Learning model to defeat the beginner Pong AI from the Gymnasium Library. implemented a simple DQN model using the PyTorch library to achieve this. Gifs of the model at different parts of training are saved in the model_gifs file.\n",
    "\n",
    "Files in this assignment:\n",
    "\n",
    "assign2 - this file, contains last run\n",
    "\n",
    "model_gifs - I was unable to make a model observer, so I incorporated the making of gifs into this assign2 file. These gifs have been saved in model_gifs\n",
    "\n",
    "\n",
    "\n",
    "Also, no my program didn't error out I ended it early on accident.\n",
    "\n",
    "## Explanation of the pong problem\n",
    "\n",
    "The Pong Problem, consists of a simple 2-player game where:\n",
    "-The left paddle is controlled by the environment/ai in relation to its difficulty level.\n",
    "-The right paddle is controlled by the Reinforcement Learning Agent\n",
    "-The victory condition is to score 21 points against the opponent\n",
    "-Points are acquired when you reflect the ball past the opponenet's paddle.\n",
    "\n",
    "\n",
    "Environmental factors:\n",
    "Steps are somewhat equivilent to frames, as in atari games player actions are understood on what frame their action is taken. \n",
    "Therefore, steps (or in my case groups of 4 steps) recieve 1 action from each player, which is reflected in the next frame.\n",
    "\n",
    "Episodes are equivilent to 1 game, and reward the sumnation of all rewards gained during itself. The episode ends when 1 side achieves 21 points. To understand the score of an episode, a negative reward indicates the model lost, with the increasing volume of negative score indicating how hard it lost, and vice versa with positive score/victory.\n",
    "\n",
    "This environment consists only of a 210 by 160 pixel image with 3 colors, which is the only output that the Model is able to observe. The model has an action space consisting of 6 possible actions, including up and down, which are by far the most important.\n",
    "\n",
    "To emulate reward, the Model is fed a reward of +1 when it scores, and -1 when the opponent scores, incentivising actions that lead to a victory, and discouraging defeat.\n",
    "\n",
    "\n",
    "To defeat pong, one should train a model on which actions result in the highest rewards based on the current state of the game, and the model must come to understand how to maximize long term rewards to optimize scoring and resisting scores on itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Problem\n",
    "\n",
    "\n",
    "## Pong\n",
    "\n",
    "Pong is one of the simplest Atari game as in the figure below. The goal of the game is winning the pingpong/tennis-like game by scoring 21 first. A player scores one point when the opponent hits the ball out of bounds or misses a hit. The right paddle is controlled by your RL agent and left paddle is controlled by a computer. \n",
    "\n",
    "![pong](https://ale.farama.org/_static/videos/environments/pong.gif)\n",
    "\n",
    "#### STEPS for Pong\n",
    "\n",
    "1. [II Problems] First, import gymnasium (if you haven't installed it, make sure install gymnasium first.\n",
    "1. [II Problems] Initialize, learn and test how the environment works.\n",
    "1. [II Problems] Explain the environment code.\n",
    "3. [III Methods] Build your own Deep Q Network (DQN). \n",
    "4. [III Methods] Explain your RL agent (DQN) with review of VFA and how it is implemented.\n",
    "5. [IV Results]  Discuss your hyperparameter search process. \n",
    "5. [IV Results]  Explain your final setup and discuss the agent's performance. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Environment code\n",
    "\n",
    "For the environment, I had to import the usual suspects (numpy, matplotlib, random, gym), along with supporting libraries such as the ale_py atati python library, as well as gymnasiums wrapper classes.\n",
    "\n",
    "In regards to the variables, I set a few variables for different training options (easy medium hard) that correspond with the difficulty number gymnasium associates them with. For this assignment, I only did easy as my GPU was having overheating issues.\n",
    "\n",
    "To create the environment, I ran the following:\n",
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"rgb_array\", difficulty=easy)\n",
    "env = ResizeObservation(env, (64, 64))\n",
    "env = GrayscaleObservation(env, keep_dim=True)\n",
    "\n",
    "The env.make() and it's inputs simply create the environment similar to the last assignment.\n",
    "The first real thing of interest was my resizing of the environment and subsequent grayscaling. I implemented these out of concern for the amount of memory used by my program, but this usage was heavily inspired by a youtube video (https://www.youtube.com/watch?v=vaVBd9H2eHE) I watched explaining these as well. These primarily came in handy for limiting the amount of memory I preallocated, allowing my older GPU to handle more.\n",
    "\n",
    "In regards to choosing 64, 64, I considered doing 84, 84 as those were the values used in the OpenAI DQN paper I read through, but I went with 64x64 as it seemed to be sufficient in the examples I saw online. In addition, I believed it would speed up training and reduce my limited memory as I am training on an older gpu. I saw an explanation stating if you think you could play the game if it was divided into information squares with divided by those numbers, it would be sufficient, which I concurred.\n",
    "\n",
    "One thing I thought was interesting was the grayscale, in which I was able to reduce the amount of channels from 3 to 1, but I still wish to do some more research on how this affected the training.\n",
    "\n",
    "Later on, similar to the last assignment, I used env.reset() to reset the pong environment along with the other env. commands, but I will not go over them too heavily in detail considering this was covered in the last assignment. I typically used the general env methods to gain information about the environment, such as action_dimensions=env.action_space.n to acquire basic information about actions. \n",
    "\n",
    "After initialization, to interact between gymnasium and pytorch, I would get the obs from env.reset(), and would pass it to a pytorch tensor. This came with the related issue that pytorch expected my channels in a different order, so throughout my code I would either use something like this:\n",
    "\n",
    "obs, info = env.reset()\n",
    "obs = obs.transpose(2, 0, 1)\n",
    "\n",
    "Or, I would use the a slightly modified method shown by the youtube video (https://www.youtube.com/watch?v=vaVBd9H2eHE)\n",
    "def proc_obs(obs):\n",
    "    obs = torch.tensor(obs,dtype=torch.float32).permute(2,0,1)\n",
    "    return obs\n",
    "\n",
    "This information would then be fed into my networks, and eventually an action would be made, and the environment would be informed.\n",
    "\n",
    "The last interaction with gymnasium to cover is rewards, which simply would award/penalize based on the assertions made in my pong explanation earlier.\n",
    "\n",
    "## ENVIRONMENT CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "# Add your code for setting up the environment\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as Green_FN\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import ale_py as ale\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from gymnasium.wrappers import ResizeObservation, GrayscaleObservation\n",
    "easy = 1\n",
    "normal = 2\n",
    "hard = 3\n",
    "\n",
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"rgb_array\", difficulty=easy)\n",
    "env = ResizeObservation(env, (64, 64))\n",
    "env = GrayscaleObservation(env, keep_dim=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"gray\">[Delete Me] add your writing: You can refer to our previous assignment.</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Methods\n",
    "\n",
    "## Describe your neural network function approximator (how many hidden unites? why?).\n",
    "\n",
    "My neural network function approximator was made up of the following:\n",
    "- 3 Convolutional Layers\n",
    "- 3 Linear nn Layers\n",
    "- An addition Linear Layer that maps the neurons to the 6 actions.\n",
    "\n",
    "Explaining Hidden units/parameters:\n",
    "\n",
    "The Convolutional Layers contained scaling channels, and I chose 1 due to the 1 color channel resulting from the grayscale. It then was scaled up to 32 through the convolutional nns. The 3 convolutional layers seemed to be enough for both 84x84 and 64x64 as the DQN paper I read, along with 2 githubs and the youtube videos all used similar setups. I went with the #of Kernalds and stride used in the youtube video as my knowledge of these values is limited, but to my understanding the kernal size is the looking glass, while stride is moving the looking glass, so leaving stride as half of the kernal size seemed apt.\n",
    "\n",
    "The linear layers, contrary to the code below (which state a default 256), is given the parameter later \"hidden_layers = 128\".\n",
    "The Deepmind paper used 512 hidden layers, and the video I watched used 256, but I felt that due to my lack of memory space, I should use 128 even if it might have more difficulty learning. In addition, pong is a simple game, so I did not think that reducing the amount of hidden layers used by other experiments would be too detrimental to my success.\n",
    "\n",
    "Lastly, the final_layer simply connects the final layer of neurons to the 6 actions. I considered making only 2 actions selectable as I don't understand the need for the other actions, but I left it be as it seemed out of scope at this time.\n",
    "\n",
    "One thing of note in my forward method, was the h = h.view(h.size(0), -1).\n",
    "This simply flattened the non batch_size dimensions into a single vector for the fully connected layers.\n",
    "\n",
    "For the calculate_convolutional_output, it was used to measure what the input for the initial fc layer should be.\n",
    "\n",
    "One additional thing, weights_init is COPIED from \"https://www.youtube.com/watch?v=vaVBd9H2eHE\" as I had never used weights before and wanted to incorporate them for learning. The code could be ran without these weights but it seemed to improve early performance.\n",
    "\n",
    "The initial weights used were kaiming normal for conv2d and xaiver normal for the linear weights. It was interesting comparing full zero weights with having these weights during my experimentation.\n",
    "\n",
    "\n",
    "\n",
    "## Explain your codes / GENERAL CODE STRUCTURE EXPLANATION\n",
    "\n",
    "For the other sections, my explanations of their codes are provided above their texts below. EX: The buffer class and the game loop both have explanations above them. Here, I will give an explanation of the entirity and structure of my code.\n",
    "\n",
    "In general, for this to work you need the following:\n",
    "\n",
    "A buffer using normal memory, a training loop that uses the gpu, an agent/model, and an environment.\n",
    "\n",
    "My code is divided into the following classes/sections:\n",
    "\n",
    "Initialization (above code that starts env and code before the training loop that sets up all of my parameters\n",
    "\n",
    "environment\n",
    "-Simply operates out of the gymnasium class, which I defined earlier. Does not have much associated code.\n",
    "-Provides ongoing states, and we provide it with our action taken\n",
    "\n",
    "py_model\n",
    "-This class initializes and operates my neural networks\n",
    "-It takes in observation and operates on it\n",
    "\n",
    "Buffer\n",
    "-This class takes in previous runs and stores them in the cpu memory for later access\n",
    "-It primarily takes in np while the pymodel takes in tensors\n",
    "\n",
    "game loop\n",
    "-This section acts as the agent, (its basically a disgusting double loop)\n",
    "-Basically has a loop of all episodes, which contains the step loop\n",
    "-Sends and recieves all information from the above classes and handels information transfer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## py_model Class explanation\n",
    "\n",
    "Already explained above, below sections will have explanations in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR THIS CLASS I LEARNED HOW TO DO THIS FROM THE YOUTUBE VIDEO LINKED BELOW\n",
    "#I DID NOT TAKE ML SO I HAD TO LEARN PYTORCH FROM SCRATCH\n",
    "#https://www.youtube.com/watch?v=vaVBd9H2eHE\n",
    "#Convolutional NN + relu maps out the features, which are then flattened into input layer\n",
    "#this flattened layer is fed into a fully connected layer\n",
    "#it is then outputted\n",
    "class py_model(nn.Module):\n",
    "    def __init__(self, action_dimension, hidden_dimensions=256, observation_shape=None):\n",
    "        super(py_model, self).__init__()\n",
    "        #THIS INITIALIZES ALL OF THE LAYERS\n",
    "        #Convolutional Layers\n",
    "        #3 Layers due to decreasing the image \n",
    "        self.convolutional_layer_1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=4, stride=2)\n",
    "\n",
    "        self.convolutional_layer_2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=4, stride=2)\n",
    "\n",
    "        self.convolutional_layer_3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2)\n",
    "\n",
    "        convulational_layer_output_size = self.calculate_convolutional_output(observation_shape)\n",
    "\n",
    "        self.fully_connected_layer_1 = nn.Linear(convulational_layer_output_size, hidden_dimensions)\n",
    "        self.fully_connected_layer_2 = nn.Linear(hidden_dimensions, hidden_dimensions)\n",
    "        self.fully_connected_layer_3 = nn.Linear(hidden_dimensions, hidden_dimensions)\n",
    "        \n",
    "        self.final_layer = nn.Linear(hidden_dimensions, action_dimension)\n",
    "        #weights initialization taken from video\n",
    "        self.apply(self.weights_init)\n",
    "    \n",
    "\n",
    "    def forward(self, h):\n",
    "        #actual running\n",
    "        h = h / 255\n",
    "        h = Green_FN.relu(self.convolutional_layer_1(h))\n",
    "        h = Green_FN.relu(self.convolutional_layer_2(h))\n",
    "        h = Green_FN.relu(self.convolutional_layer_3(h))\n",
    "        h = h.view(h.size(0), -1)\n",
    "        h = Green_FN.relu(self.fully_connected_layer_1(h))\n",
    "        h = Green_FN.relu(self.fully_connected_layer_2(h))\n",
    "        h = Green_FN.relu(self.fully_connected_layer_3(h))\n",
    "        output = self.final_layer(h)\n",
    "        return output\n",
    "    #--------------------------------------------------------------------------------------------\n",
    "    #These methods were mostly taken and modified from the previously mentioned from the youtube video, instead of written\n",
    "    def calculate_convolutional_output(self, observation_shape):\n",
    "        h = torch.zeros(1, *observation_shape)\n",
    "        h = Green_FN.relu(self.convolutional_layer_1(h))\n",
    "        h = Green_FN.relu(self.convolutional_layer_2(h))\n",
    "        h = Green_FN.relu(self.convolutional_layer_3(h))\n",
    "        return h.view(-1).shape[0]\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffer Class Explanation\n",
    "\n",
    "This buffer class, as mentioned before, stores the historical runs of the model and keeps the gpu from being overloaded with memory.\n",
    "By maintaining a structued memory of past states, rewards, outcomes, and actionsm the model can go back to old data and reuse it.\n",
    "\n",
    "def __init__(self, max_size, input_shape, device='cpu')\n",
    "\n",
    "This method initializes the buffer and preallocates all of the memory required\n",
    "input shape takes the shape of the grayscale image, device tells what type of memory is to be allocated, and max_size\n",
    "refers to the maximum amount of transitions that can be stored in the buffer.\n",
    "\n",
    "In this method, we preallocate for \n",
    "past states, next states, actions taken, rewards, and if an episode ended.\n",
    "\n",
    "def determine_samplability()\n",
    "-this method simply returns if more than 10x the batch_size experiences have been stored\n",
    "-this ensure a sufficient base amount of exploration is done before we even are able to call upon our past experiences\n",
    "\n",
    "\n",
    "def store_transition(self, state, action, reward, next_state, game_complete)\n",
    "This method stores an experience in to the replay buffer.\n",
    "We take in our self, state, action, reward, next_state, game_complete\n",
    "which is what we observed in our state.\n",
    "\n",
    "The structure flows like this\n",
    "overwrite old experience\n",
    "stores memory\n",
    "increments memory index counter\n",
    "\n",
    "\n",
    "def sample_buffer(self, batch_size)\n",
    "This method samples a batch of random past experiences according to the number batch_size.\n",
    "\n",
    "It does this through the following execution steps:\n",
    "if buffer isnt full, max_memory is set to the minimum of mem counter to see the valid samplable range\n",
    "randomly picks indices of experiences in memory\n",
    "retrieves all of the associated experience stuff from it in memory\n",
    "moves it to the gpu and returns it for the game loop to use for action determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------REPLAY BUFFER CLASS---------------------\n",
    "#I used the tutorial from-----https://www.youtube.com/watch?v=vaVBd9H2eHE\n",
    "#I did not know how to code this, so I learned from this video.\n",
    "#I did not copy this section, but I rewrote it after watching and learning from the video\n",
    "#again, i have never done rl before so I had to start somewhere\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, input_shape, device='cpu'):\n",
    "        #MEMORY ALLOCATION TO NORMAL RAM\n",
    "        self.memory_size = max_size\n",
    "        self.memory_counter = 0\n",
    "        #Creates memory and fills with zeros initially, multidimensional\n",
    "        self.state_memory = np.zeros((self.memory_size, *input_shape), dtype=np.uint8)\n",
    "        #creates next state memory and fills it with zeros, multidimensional\n",
    "        self.next_state_memory = np.zeros((self.memory_size, *input_shape), dtype=np.uint8)\n",
    "        #Action and reward memory, single dimensional\n",
    "        self.action_memory = np.zeros(self.memory_size, dtype=np.uint8)\n",
    "        self.reward_memory = np.zeros(self.memory_size, dtype=np.float32)\n",
    "        #Terminal memory\n",
    "        self.terminal_memory = np.zeros(self.memory_size, dtype=bool)\n",
    "        #Setting method device as cpu\n",
    "        self.device = device #Basically, all this stuff is stored on the normal RAM, not gpu RAM\n",
    "\n",
    "        \n",
    "    def determine_sampleability(self, batch_size):\n",
    "        if self.memory_counter > (batch_size * 10):\n",
    "            return True\n",
    "            print(\"HELLO\")\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        #actions - action_memory\n",
    "        #rewards - reward memory\n",
    "        #next_states - next_state_memory\n",
    "        #state - state_memory\n",
    "        #completed_games - terminal_memory - game_complete - when game is done\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, game_complete):\n",
    "        #Calculating how many things I have stored in memory\n",
    "        #This rewrites the buffer after it reaches a certain point\n",
    "        #It preallocates, and writes over when it get to that point\n",
    "        memory_index = self.memory_counter % self.memory_size\n",
    "        #Current state is where it is in memory, next state is the next area in memory past each mem_size\n",
    "        self.state_memory[memory_index] = state.cpu().numpy().astype(np.uint8)\n",
    "        self.next_state_memory[memory_index] = next_state.cpu().numpy().astype(np.uint8)\n",
    "\n",
    "        #Doing the above for the action/reward/terminal_memory as well\n",
    "        self.action_memory[memory_index] = torch.tensor(action).detach().cpu()\n",
    "        self.reward_memory[memory_index] = reward\n",
    "        self.terminal_memory[memory_index] = game_complete\n",
    "\n",
    "        #iterating the memory counter, we have gone to the next state stored in memory\n",
    "        self.memory_counter +=1 \n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        #retrieving a random batch of past experiences\n",
    "        #ensures it doesnt retrieve from unallocated memory\n",
    "        max_memory = min(self.memory_counter, self.memory_size)\n",
    "        #this ensure we randomly choose different parts of memory\n",
    "        batch = np.random.choice(max_memory, batch_size)\n",
    "\n",
    "        #here, i \n",
    "        states = self.state_memory[batch]\n",
    "        next_states = self.next_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        completed_games = self.terminal_memory[batch]\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        completed_games = torch.tensor(completed_games, dtype=torch.bool).to(self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, completed_games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Misc Methods\n",
    "\n",
    "gradual update provides a soft update to the target network, creating stability\n",
    "\n",
    "proc_obs basically prevents me from having to permute every time obs is mentioned.\n",
    "\n",
    "Gym -> Torch has a weird interaction where gym's channel/length/stuff is ordered in the oppisate, so this methods moves it around and changes the gym numpy variable to a torch variable. This was made due to gym obs being given so often in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradual_update(target, source, tau=0.005):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def proc_obs(obs):\n",
    "    obs = torch.tensor(obs,dtype=torch.float32).permute(2,0,1)\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of params and various declarations\n",
    "\n",
    "This section is mostly covered in the hyper parameters section, so I will go over what is not covered over there.\n",
    "\n",
    "self_repeat was left over from an earlier version\n",
    "the initial env.reset() and obs.transpose are leftover from some testing\n",
    "\n",
    "episode_steps is tracking for one of the charts, and at one point i had it prevent episodes from using more than 10k steps,\n",
    "because of an earlier code bug that i resolved.\n",
    "\n",
    "frame_action_repeat is explained below in the loop, it is used for repeating an action for n frames, making the paddle more consistant\n",
    "\n",
    "device is set to cuda because i am using the gpu for training in this section\n",
    "\n",
    "the 4 arrays are for tracking and briefly covered below\n",
    "\n",
    "print(f'loaded model on device {device}') was testing as a result of earlier errors with my nvidia drivers\n",
    "\n",
    "Everything else here is covered in the hyperparams explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model on device cuda\n"
     ]
    }
   ],
   "source": [
    "#Writing the actual algorithm\n",
    "obs, info = env.reset()\n",
    "obs = obs.transpose(2, 0, 1)\n",
    "self_repeat = \"\"\n",
    "batch_size = 64\n",
    "epsilon = 1\n",
    "min_epsilon = .1\n",
    "epsilon_decay = .998\n",
    "hidden_layers = 128\n",
    "learning_rate = 0.0001\n",
    "episodes = 10000\n",
    "episode_steps = 0\n",
    "max_episode_steps = 10000\n",
    "frame_action_repeat = 4\n",
    "device = 'cuda'\n",
    "gamma = 0.99\n",
    "print(f'Loaded model on device {device}')\n",
    "episode_rewards = []\n",
    "episode_steps_list = []\n",
    "all_actions_taken = []\n",
    "rolling_avg_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Training loops/code\n",
    "\n",
    "This section is about to be extremely long due to the complexity of the model, and my inability to split it without breaking it.\n",
    "Next time I am 100% making a seperate agent class to allow for better and clearer execution flow.\n",
    "\n",
    "First of all, before the loop, we have a bit more setup.\n",
    "We declare memory which is our cpu memory to store our buffer, as well as the following\n",
    "\n",
    "memory \n",
    "model (our actual model)\n",
    "target model (target model)\n",
    "optimizer (basic adam optimizer I saw used in deepmind and openai papers. not really sure what it does but thought id mess with it)\n",
    "we also set steps taken, which is a general tracker for the loop\n",
    "\n",
    "\n",
    "for the actual loop it begins with \n",
    "for episode in range (episodes):\n",
    "    game_complete=False\n",
    "    reward_from_this_episode=0\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    obs = proc_obs(obs)\n",
    "\n",
    "In this section, we declare our episode loop which loops 1 time per episode,\n",
    "set our game_complete flag to false, which lets us know when the pong match is over (pong match = episode)\n",
    "we also get our reward from the episode and set it to zero.\n",
    "\n",
    "Lastly, we reset our environment and ensure obs is ready for gpu usage.\n",
    "\n",
    "    -------------------\n",
    "    Next, we enter our game loop\n",
    "    \n",
    "    while game_complete is False and episode_steps<max_episode_steps:\n",
    "    \n",
    "    This loop occurs every episode, and contains our logic for episodes.\n",
    "    We let this run until the episode finishes, or we use 10k steps, which used to be a bug in the code.\n",
    "\n",
    "    We then use our greedy epsilon to determine if we use known knowledge or use known knowledge.\n",
    "    We add whatever action we used to our array for one of our plots, then move on.\n",
    "    \n",
    "    We set our reward to zero, then begin a tertiary loop.\n",
    "        for i in range(frame_action_repeat):\n",
    "                t = 0\n",
    "                next_obs, t, game_complete, truncated, info = env.step(action=action)\n",
    "                next_obs = proc_obs(next_obs)\n",
    "                reward+= t\n",
    "                if (game_complete):\n",
    "                    break\n",
    "\n",
    "        In this tertiary loop, we basically have weaker steps for 4 steps, reducing compute cost.\n",
    "\n",
    "    following this, we store our experience, and iterate our important markers, like steps taken and episode_steps\n",
    "    \n",
    "    memory.store_transition(obs, action, reward, next_obs, game_complete)\n",
    "\n",
    "        obs = next_obs\n",
    "        reward_from_this_episode += reward\n",
    "        episode_steps +=1\n",
    "        steps_taken +=1\n",
    "\n",
    "\n",
    "    If enough data is present in our buffer, we then train the model based on this data.\n",
    "    We do this by retreiving a batch of previous data if allowed by our determine samplability method.\n",
    "    We compute the q vals of the current states\n",
    "    Compute q vals in the target model\n",
    "    find best action for next state\n",
    "    retrieve next q from target model\n",
    "    compute our bellman target\n",
    "    We also adjust our target model every 4 steps\n",
    "    \n",
    "    We then do a gradual update to the target model depending on if we are on every 4th step or not.\n",
    "\n",
    "The game loop then ends, and we are back in the episode loop.\n",
    "\n",
    "We reduce our epsilon, \n",
    "add values for our different performance measuring metrics,\n",
    "compute rolling average for the matplotlib,\n",
    "\n",
    "In the output box, on thing of note is that episode steps are reset.\n",
    "\n",
    "## Explanation of Plots\n",
    "\n",
    "For the plots, all that you really need to know is that the following arrays pick up data from various\n",
    "parts of the loop, and are thrown into various matplots. Its really simple and very apparant when it happens\n",
    "The plot code is centralized near the end of the episode loop, but before the early end condition is checked and the episode steps are reset.\n",
    "These plots update once per episode.\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "episode_steps_list = []\n",
    "\n",
    "all_actions_taken = []\n",
    "\n",
    "rolling_avg_rewards = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ae9d67a7394fef8b5d9d58e4c7a696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training gameplay as pong_training_episode_0.gif\n",
      "Frames in gif: 764\n",
      "Saved training gameplay as pong_training_episode_1000.gif\n",
      "Frames in gif: 3880\n",
      "Saved training gameplay as pong_training_episode_2000.gif\n",
      "Frames in gif: 3328\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frame_action_repeat):\n\u001b[1;32m     35\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 36\u001b[0m     next_obs, t, game_complete, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     next_obs \u001b[38;5;241m=\u001b[39m proc_obs(next_obs)\n\u001b[1;32m     38\u001b[0m     reward\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/gymnasium/core.py:560\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    558\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    559\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/gymnasium/core.py:560\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    558\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    559\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/gymnasium/core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/ale_py/env.py:311\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    309\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[0;32m--> 311\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43male\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    314\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_box = widgets.Output()\n",
    "display(output_box)\n",
    "device = 'cuda'\n",
    "memory = ReplayBuffer(max_size=1000000, input_shape=obs.shape, device=device)\n",
    "\n",
    "model = py_model(action_dimension=env.action_space.n, hidden_dimensions=hidden_layers, observation_shape=obs.shape).to(device)\n",
    "\n",
    "target_model = py_model(action_dimension=env.action_space.n, hidden_dimensions=hidden_layers, observation_shape=obs.shape).to(device)\n",
    "\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "steps_taken=0\n",
    "\n",
    "for episode in range (episodes):\n",
    "    game_complete=False\n",
    "    reward_from_this_episode=0\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    obs = proc_obs(obs)\n",
    "    \n",
    "    while game_complete is False and episode_steps<max_episode_steps:\n",
    "        #Random action for exploring\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        #exploiting our knowledge\n",
    "        else:\n",
    "            q = model.forward(obs.unsqueeze(0).to(device))[0]\n",
    "            action = torch.argmax(q, dim=-1).item()\n",
    "        all_actions_taken.append(action)\n",
    "        reward = 0\n",
    "\n",
    "        for i in range(frame_action_repeat):\n",
    "            t = 0\n",
    "            next_obs, t, game_complete, truncated, info = env.step(action=action)\n",
    "            next_obs = proc_obs(next_obs)\n",
    "            reward+= t\n",
    "            if (game_complete):\n",
    "                break\n",
    "\n",
    "        memory.store_transition(obs, action, reward, next_obs, game_complete)\n",
    "\n",
    "        obs = next_obs\n",
    "        reward_from_this_episode += reward\n",
    "        episode_steps +=1\n",
    "        steps_taken +=1\n",
    "\n",
    "        if memory.determine_sampleability(batch_size):\n",
    "            states, actions, rewards, next_states, completed_games = memory.sample_buffer(batch_size)\n",
    "            \n",
    "            completed_games = completed_games.unsqueeze(1).float()\n",
    "            \n",
    "            q = model(states)\n",
    "            \n",
    "            actions = actions.unsqueeze(1).long()\n",
    "\n",
    "            qsa_batch = q.gather(1, actions)\n",
    "            \n",
    "            next_actions = torch.argmax(model(next_states), dim=1, keepdim=True)\n",
    "            \n",
    "            next_q = target_model(next_states).gather(1, next_actions)\n",
    "            \n",
    "            target_b = rewards.unsqueeze(1) + (1-completed_games) * gamma * next_q\n",
    "\n",
    "            loss = Green_FN.mse_loss(qsa_batch, target_b.detach())\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if steps_taken % 4 == 0:\n",
    "                gradual_update(target_model, model)\n",
    "    \"\"\"    \n",
    "    print(\"-----------\")\n",
    "    print(f\"Episode #{episode}\")\n",
    "    print(f\"Steps taken this episode {episode_steps}\")\n",
    "    print(f\"Episode reward: {reward_from_this_episode}\")\n",
    "    print(f\"Total Steps Taken: {steps_taken}\")\n",
    "    print(f\"Current Epsilon Value: {epsilon}\")\n",
    "    print(\"-----------\")\n",
    "    \"\"\"\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon*= epsilon_decay\n",
    "    episode_rewards.append(reward_from_this_episode)\n",
    "    episode_steps_list.append(episode_steps)\n",
    "\n",
    "    if len(episode_rewards) >= 100:\n",
    "        rolling_avg_rewards.append(np.mean(episode_rewards[-100:]))\n",
    "    else:\n",
    "        rolling_avg_rewards.append(np.mean(episode_rewards))  # Use all available data initially\n",
    "    latest_rolling_avg = rolling_avg_rewards[-1]  \n",
    "    with output_box:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Completed Episode: {episode}\")\n",
    "        print(f\"Steps this Episode: {episode_steps}\")\n",
    "        print(f\"Episode Reward: {reward_from_this_episode}\")\n",
    "\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        axs[0, 0].plot(episode_rewards, label=\"Reward Per Episode\", color=\"blue\")\n",
    "        axs[0, 0].set_title(\"Reward Trend Over Time\")\n",
    "        axs[0, 0].set_xlabel(\"Episodes\")\n",
    "        axs[0, 0].set_ylabel(\"Reward\")\n",
    "        axs[0, 0].legend()\n",
    "        axs[0, 1].plot(episode_steps_list, label=\"Steps Per Episode\", color=\"green\")\n",
    "        axs[0, 1].set_title(\"Steps Per Episode\")\n",
    "        axs[0, 1].set_xlabel(\"Episodes\")\n",
    "        axs[0, 1].set_ylabel(\"Steps\")\n",
    "        axs[0, 1].legend()\n",
    "        axs[1, 0].plot(rolling_avg_rewards, label=\"Last 100 Episode Reward Avg\", color=\"red\")\n",
    "        axs[1, 0].set_title(\"Last 100 Episode Reward Average\")\n",
    "        axs[1, 0].set_xlabel(\"Episodes\")\n",
    "        axs[1, 0].set_ylabel(\"Average Reward\")\n",
    "        axs[1, 0].legend()\n",
    "        axs[1, 1].bar(range(env.action_space.n), np.bincount(all_actions_taken, minlength=env.action_space.n), color=\"purple\")\n",
    "        axs[1, 1].set_title(\"Action Distribution\")\n",
    "        axs[1, 1].set_xlabel(\"Action\")\n",
    "        axs[1, 1].set_ylabel(\"Count\")\n",
    "        axs[1, 1].set_xticks(range(env.action_space.n))\n",
    "        axs[1, 1].legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        episode_steps = 0      \n",
    "        \n",
    "    if episode % 1000 == 0:\n",
    "        frames = []\n",
    "    \n",
    "        # Reset environment for GIF recording\n",
    "        obs, info = env.reset()\n",
    "        obs = proc_obs(obs)\n",
    "        game_complete = False\n",
    "        steps_taken_gif = 0  # Track steps during GIF recording\n",
    "    \n",
    "        while not game_complete:\n",
    "            with torch.no_grad():  \n",
    "                q = model.forward(obs.unsqueeze(0).to(device))[0]\n",
    "                action = torch.argmax(q, dim=-1).item()\n",
    "    \n",
    "            reward = 0\n",
    "            for _ in range(frame_action_repeat):  # Ensure action repeat is applied\n",
    "                next_obs, t, game_complete, truncated, info = env.step(action)\n",
    "                next_obs = proc_obs(next_obs)\n",
    "                reward += t\n",
    "                frames.append(env.render())  # Capture frame at each step\n",
    "\n",
    "            if game_complete:\n",
    "                break  # Stop recording if the game ends\n",
    "        \n",
    "            obs = next_obs  # Move to next observation\n",
    "            steps_taken_gif += 1  # Increment step count\n",
    "    \n",
    "            # Ensure the model updates every 4 steps, just like in training\n",
    "            if steps_taken_gif % 4 == 0:\n",
    "                gradual_update(target_model, model)\n",
    "    \n",
    "        # Save GIF\n",
    "        import imageio\n",
    "        gif_path = f\"pong_training_episode_{episode}.gif\"\n",
    "        imageio.mimsave(gif_path, frames, duration=0.05)\n",
    "        print(f\"Saved training gameplay as {gif_path}\")\n",
    "        print(f\"Frames in gif: {len(frames)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV - Results\n",
    "\n",
    "- Describe the choice of your hyper-parameters for $\\gamma$, $\\epsilon$, the learning rates $\\rho$'s, and the number of hiddend units (or other NN hyper-parameters). \n",
    "  - Run experiments to find good hyper-parameters\n",
    "  - Show the experimental outputs to show the process of your selection\n",
    "- Visualize the results and explain outputs \n",
    "  - Run the codes and tell me what you observe\n",
    "  - Add more visualizations to enrich your explanation.\n",
    "    - Hint: example visualization can be the reward/return curve, win/lose plot, score plot, etc. \n",
    "    -       Feel free to try new plots if you want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below code was used because the training i saved for you to view ended before it reached the end condition.\n",
    "## It still successfully beat the pong many times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters section: Explanation and Code\n",
    "## Note: I do not have code for hyperparams testing, I did it manually for some of it in previous iterations, explanations below\n",
    "\n",
    "Explain your previous hyperparameters, results and give a few snippets showing hyperparam testing without writing any actual hyperparam tests.\n",
    "\n",
    "I'll explain how I came to/tested the following parameters:\n",
    "batch_size = 64\n",
    "-For batch size, I tried both 32 and 64 on a few runs, and I noticed that the average would increase to -15 a few hundred episodes quicker, so I stuck with it. I originally used 32 as I was worried about memory and I saw it as the default for chatgpt, but I changed it when my memory issues were resolved after my buffer implementation.\n",
    "\n",
    "epsilon = 1\n",
    "-Starting at 1 for epsilon seemed completely logical.\n",
    "\n",
    "min_epsilon = .1\n",
    "-For min epsilon, I chose .1 as I wanted the model to still retain adaptability after its early learning stages. I tried 0 but obviously it stopped improving after 2200 episodes so I added .1 to it.\n",
    "\n",
    "epsilon_decay = .998\n",
    "-For epsilon decay, I initially went with .99 as shown in the previously mentioned youtube video, but I was having issues with my model\n",
    "getting stuck around -15. I changed it to slow the epsilon growth, which meant it wouldn't hit min_epsilon for over 1000 episodes, which when given significant training time, lead to a really strong growth.\n",
    "\n",
    "hidden_layers = 128\n",
    "-For this parameter, it was chosen simply due to a early concern for memory, and it being a power of 2. I could probably change it to 256 and improve my training, but my training seemed to be performing just fine at the moment. I think its a good number considering the deep_mind paper used 512 and had much better resources than me (I think I have 8gb of gpuram)\n",
    "\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n",
    "episodes = 35000\n",
    "This number was set arbitrarily as a result of the time it took to do my episodes. My episodes on average take 2.46 seconds, so I figured I would try to make my training last exactly 1 day. It seems like 5000 episodes as shown in the current training is sufficient to win games about 50% of the time, so if I were to do the full training it would likely end with a more succesfull model.\n",
    "\n",
    "(Note i changed this to 10000 as i needed to retrain before the deadline.\n",
    "\n",
    "frame_action_repeat = 4\n",
    "This param forces am action to repeat for 4 frames inside of the loop, more accurately representing how atari games worked with their slower fps. (I also did this to cheat training time) It was a shown param in the video and I found it to be pretty interesting.\n",
    "\n",
    "gamma = 0.99\n",
    "This was done to make the engine heavily consider long term rewards. Never really had any issues with this value, and I used it as my default in my last assignment I believe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization/test\n",
    "\n",
    "Due to issues I had with creating a model_tester, I instead created gifs of every 1000 episodes and put it in a file called model_gifs.\n",
    "This program will create gifs every 1000 episodes in the same directory it is in.\n",
    "\n",
    "## As seen above many runs succesfully beat the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Observation of learning results and plots\n",
    "\n",
    "Completed Episode: 5126\n",
    "Steps this Episode: 865\n",
    "Episode Reward: 4.0\n",
    "\n",
    "The above is basic print text that repeats after every episode. All it does is display the last episode completed, # of steps this episode, and shows the reward. the episode reward of 4 shows that I succesfully completed the assignment.\n",
    "\n",
    "The 4 plots are the following\n",
    "\n",
    "Reward trend over time:\n",
    "This plot shows every episodes reward plotted on a basic episodexreward table.\n",
    "It aptly shows the most important parts of this experiment, and that the first successful victory occured at episode 1700~\n",
    "\n",
    "Steps per episode:\n",
    "This plot shows the steps per episode, which I thought would be an interesting metric to observe. My predictions for this would be that as the reward each episode approaches 0, the steps per run would increase, after which they would decrease.\n",
    "As you can see, when my reward gains began to plateau around 0 average, this metric also began to stabilize.\n",
    "\n",
    "Last 100 episode reward average:\n",
    "This metric was selected as I wanted a smoother and more clear understanding of how the model was progressing.\n",
    "It took the last 100 episodes as stored in rolling_avg_rewards and averaged them, creating a slow to change but\n",
    "consistantly accurate performance metric. \n",
    "\n",
    "It seems that my model seemed to slow down its progress significantly around the 4000 episode mark, winning games about 50% of the time.\n",
    "\n",
    "\n",
    "Action distribution:\n",
    "This was the last metric I thought would be interesting, as it provided an insight into how the action selection occured. This metric was more useful in the actual running, as you could see the leveled actions slowly start to favor 3 and 5, which are up and down respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Conclusions\n",
    "\n",
    "Discuss the challenges or somethat that you learned. \n",
    "If you have any suggestion about the assignment, you can write about it. \n",
    "\n",
    "## My conclusions are below\n",
    "\n",
    "For this assignment, I have the following conclusions.\n",
    "\n",
    "Reinforcement learning is hard. Very hard. I had a ton of trouble with environment setup for this assignment and a lot of trouble getting the memory to work consistently. \n",
    "\n",
    "Another thing: Performance increases when found are massive. Moving the buffer out of my gpu memory increased the buffer space I could hold by 1000x. In addition, having adequate buffer size masively increased performance. I was really happy when instead of taking a day to train 5000 episodes like I heard some of my classmates talking about, my model took 1 hour per 1450ish episodes.\n",
    "\n",
    "I learned a ton. At the start of this, I had never touched pytorch, and never truley felt like I had succesfully written a ml model. This is the first time I have actually had one come together and perform somewhat well.\n",
    "\n",
    "Having a stable linux environment helps a massive amount. I wiped my computer drive and installed linux mint, which let me have a really easy to manage docker environment. This fixed 90% of my environment instantly and allowed me to focus on coding, and I got to experience using my own GPU for machine learning for the first time. The feeling of going to sleep at 4am and waking up at 10am to 6500 episode being done and your model outperforming itself by a ton is a fantastic feeling.\n",
    "\n",
    "Lastly, I enjoyed the assignment but I likely started too late. If I were to start earlier, I would have likely been able to achieve the extra credit points and could have explored more models, optimizers, and weights, instead of using ones suggested by youtube videos and research papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "You are required to submit three files. \n",
    "1. This notebook with complete writing. **assign2**\n",
    "2. Stored keras or pytorch model(s) (if you complete the extra credit assignment, you should submit 2 model files) **t**\n",
    "3. Another notebook that load the model and test the trained model on the pong environment. **model_tester.py**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "For this assignment, the grading rubric is a bit different. Please check it carefully. \n",
    "\n",
    "\n",
    "points | | description\n",
    "--|--|:--\n",
    "5 | Overview| states the objective and the appraoch \n",
    "20 | Problem | \n",
    " |10| Code for setting up the environment\n",
    " | 5| Explanation of Pong problem\n",
    " | 5| explanation of the codes to use Pong from Gymnasium\n",
    "25 | Methods | \n",
    " |15| Your DQN agent codes\n",
    " |10| Explanation of your implementation and DQN method\n",
    "35 | Results \n",
    " | 5| Codes for hyperparameter search \n",
    " |10| Experimental outputs that show the choice of parameters. Explanation of how do you choose them?\n",
    " |10| Visualization of learning and learned agent\n",
    " |10| Observations and analysis of learning results and plots\n",
    "5 | Conclusions \n",
    "10 | Successful test of the submitted model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
