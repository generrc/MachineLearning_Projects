{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKe6sP8WSifG"
   },
   "source": [
    "# Federated Learning with Trustrank adjusted learning\n",
    "\n",
    "This model uses a trustrank-like system to detect botnet-like behavior. The aim is to implement a simplified federated learning network that employs a TrustRank system to identify potential Malware/Botnet impacted devices using Federated Learning, and restricting them on a Trust rating based on their scores achieved in the Federated Training.\n",
    "\n",
    "The trustrank score accounts for how often each device was interpreted as being infected by the model using a variety of metrics that will be explained/implemented later.\n",
    "\n",
    "In a network setting, these trustrank scores would implement differing levels of restrictions in regards to: Allowed traffic throughput, packet queue retention, and firewall access.\n",
    "\n",
    "**Will not be considering adversarial clients, we will be assuming all clients are contributing to the model in good faith.**\n",
    "\n",
    "An essential part of this project is portraying the hypothetical impacts that restricting botnets on the network will have. In addition, providing a Proof of concept alert to the network switch would do an effective job at playing this concept out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Yv9m8sMV2Hn"
   },
   "source": [
    "# TODO\n",
    "\n",
    "Restart search for a dataset to attempt to simulate different nodes:<br>\n",
    "  This dataset must have differing nodes inside of the dataset to allow for detection of nodes that posses malware.<br>\n",
    "    Possible datasets:<br>\n",
    "      IoT-23 (already investigated, would work but its a fuckton of data)<br>\n",
    "      bot-iot<br>\n",
    "      **N-BaIoT** (currently implemented but not working well.)<br>\n",
    "      TON_IoT Dataset<br>\n",
    "      CICIDS2017 (??????)<br>\n",
    "\n",
    "Implement a simple federated learning algorithm acting for all of the nodes analyzing each other, determining which other nodes have the most botnet like action.\n",
    "\n",
    "Implement a trustrank system using the machine learning results that determines the relevant trust level of each node. Implement this with a dqn test case\n",
    "\n",
    "Implement the hyperparameter pipeline, most likely will run this section locally on my computer rather than running this section, so likely will comment this section out.\n",
    "  Include a TXT section explaining there params and include a file that allows for importing the results. Use pipeline for this.\n",
    "\n",
    "Implement the actual training loop using the training + all other classes and definitions.\n",
    "\n",
    "Create the plots and graphs for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "onSCIM2QVQnd"
   },
   "outputs": [],
   "source": [
    "# Dataset cloning + importing of relevant coding libraries\n",
    "\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7r0DCobPHYs"
   },
   "source": [
    "## Data Preprocessing for Supervised Learning\n",
    "- Skip during demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L2itEUqiS_Bi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and grouping CSV files by device...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 92/92 [00:52<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data and saving per-device CSVs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 12/12 [06:49<00:00, 34.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving combined train/test files...\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to your N-BaIoT CSV files\n",
    "DATA_DIR = \"N-BaIoT\"\n",
    "OUTPUT_FILES = [\n",
    "    \"combined_train.csv\",\n",
    "    \"combined_test.csv\"\n",
    "] + [f\"{i}_{suffix}.csv\" for i in range(1, 10) for suffix in [\"train\", \"test\"]]  # adjust range if needed\n",
    "\n",
    "# Clean up old files\n",
    "for f in OUTPUT_FILES:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "\n",
    "tqdm.pandas()\n",
    "csv_files = glob.glob(os.path.join(DATA_DIR, \"*.csv\"))\n",
    "device_data = {}\n",
    "\n",
    "# Label extractor\n",
    "def extract_label_from_filename(filename):\n",
    "    name = os.path.basename(filename).lower()\n",
    "    if \"benign\" in name:\n",
    "        return \"benign\"\n",
    "    else:\n",
    "        return \"malicious\"  # assumes everything else is an attack\n",
    "\n",
    "print(\"Reading and grouping CSV files by device...\")\n",
    "for file in tqdm(csv_files):\n",
    "    base = os.path.basename(file)\n",
    "    device_id = base.split('.')[0]\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Add label from filename\n",
    "    df[\"label\"] = extract_label_from_filename(base)\n",
    "\n",
    "    # Optional: Drop metadata columns\n",
    "    df = df.drop(columns=[col for col in df.columns if \"device\" in col.lower() or \"feature\" in col.lower() or \"file\" in col.lower()], errors='ignore')\n",
    "\n",
    "    if device_id not in device_data:\n",
    "        device_data[device_id] = []\n",
    "    device_data[device_id].append(df)\n",
    "\n",
    "combined_train = []\n",
    "combined_test = []\n",
    "\n",
    "print(\"Splitting data and saving per-device CSVs...\")\n",
    "for device_id in tqdm(device_data):\n",
    "    device_df = pd.concat(device_data[device_id], ignore_index=True)\n",
    "    \n",
    "    # Stratified split by label\n",
    "    train_df, test_df = train_test_split(device_df, test_size=0.2, random_state=42, shuffle=True, stratify=device_df[\"label\"])\n",
    "\n",
    "    train_df.to_csv(f\"{device_id}_train.csv\", index=False)\n",
    "    test_df.to_csv(f\"{device_id}_test.csv\", index=False)\n",
    "\n",
    "    combined_train.append(train_df)\n",
    "    combined_test.append(test_df)\n",
    "\n",
    "print(\"Saving combined train/test files...\")\n",
    "pd.concat(combined_train, ignore_index=True).to_csv(\"combined_train.csv\", index=False)\n",
    "pd.concat(combined_test, ignore_index=True).to_csv(\"combined_test.csv\", index=False)\n",
    "\n",
    "print(\"All done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qjH2RdJYg8rZ"
   },
   "outputs": [],
   "source": [
    "# SupLearn Imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bRBMEkIkVDtE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and test data...\n",
      "Handling missing values...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- HH_L0.01_std\nFeature names seen at fit time, yet now missing:\n- label\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m imputer = SimpleImputer(strategy=\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     22\u001b[39m X_train = imputer.fit_transform(X_train)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m X_test = \u001b[43mimputer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Feature scaling\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mScaling features...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/sklearn/utils/_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/sklearn/impute/_base.py:607\u001b[39m, in \u001b[36mSimpleImputer.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    592\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Impute all missing values in `X`.\u001b[39;00m\n\u001b[32m    593\u001b[39m \n\u001b[32m    594\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    603\u001b[39m \u001b[33;03m    `X` with imputed values.\u001b[39;00m\n\u001b[32m    604\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    605\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    608\u001b[39m statistics = \u001b[38;5;28mself\u001b[39m.statistics_\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.shape[\u001b[32m1\u001b[39m] != statistics.shape[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/sklearn/impute/_base.py:363\u001b[39m, in \u001b[36mSimpleImputer._validate_input\u001b[39m\u001b[34m(self, X, in_fit)\u001b[39m\n\u001b[32m    361\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ve\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m in_fit:\n\u001b[32m    366\u001b[39m     \u001b[38;5;66;03m# Use the dtype seen in `fit` for non-`fit` conversion\u001b[39;00m\n\u001b[32m    367\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_dtype = X.dtype\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/sklearn/impute/_base.py:344\u001b[39m, in \u001b[36mSimpleImputer._validate_input\u001b[39m\u001b[34m(self, X, in_fit)\u001b[39m\n\u001b[32m    341\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcould not convert\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/sklearn/utils/validation.py:2919\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2835\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2836\u001b[39m     _estimator,\n\u001b[32m   2837\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2843\u001b[39m     **check_params,\n\u001b[32m   2844\u001b[39m ):\n\u001b[32m   2845\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2846\u001b[39m \n\u001b[32m   2847\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2917\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2918\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2919\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2920\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/sklearn/utils/validation.py:2777\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2774\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2775\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- HH_L0.01_std\nFeature names seen at fit time, yet now missing:\n- label\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading training and test data...\")\n",
    "train_df = pd.read_csv(\"combined_train.csv\", low_memory=False)\n",
    "train_df[\"label\"] = train_df[\"label\"].map({\"benign\": 0, \"malicious\": 1})\n",
    "train_df = train_df.loc[:, ~train_df.columns.str.contains('^Unnamed')]\n",
    "test_df = pd.read_csv(\"combined_test.csv\",low_memory=False)\n",
    "test_df[\"label\"] = test_df[\"label\"].map({\"benign\": 0, \"malicious\": 1})\n",
    "test_df = test_df.loc[:, ~test_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_df.drop(\"label\", axis=1)\n",
    "t_train = train_df[\"label\"]\n",
    "X_test = test_df.drop(\"label\", axis=1)\n",
    "t_test = test_df[\"label\"]\n",
    "\n",
    "print(\"Handling missing values...\")\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Feature scaling\n",
    "print(\"Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Feature scaling\n",
    "print(\"Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define baseline models\n",
    "models = {\n",
    "    #\"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1),\n",
    "    \"SVM\": SVC(),\n",
    "    \"MLP\": MLPClassifier(max_iter=500)\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "print(\"Training and evaluating models...\")\n",
    "for name in tqdm(models):\n",
    "    model = models[name]\n",
    "    model.fit(X_train, t_train)\n",
    "    preds = model.predict(X_test)\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(classification_report(t_test, preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and test data...\n",
      "Top 10 correlated features: ['HH_L0.01_std', 'HH_L0.1_std', 'HpHp_L0.01_std', 'MI_dir_L0.1_weight', 'H_L0.1_weight', 'MI_dir_L1_weight', 'H_L1_weight', 'MI_dir_L3_weight', 'H_L3_weight', 'MI_dir_L5_weight']\n",
      "Handling missing values...\n",
      "Scaling features...\n",
      "Training and evaluating models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████▋                                                       | 1/3 [00:57<01:55, 57.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9995    0.9998    111188\n",
      "           1     1.0000    1.0000    1.0000   1301381\n",
      "\n",
      "    accuracy                         1.0000   1412569\n",
      "   macro avg     1.0000    0.9998    0.9999   1412569\n",
      "weighted avg     1.0000    1.0000    1.0000   1412569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load data\n",
    "print(\"Loading training and test data...\")\n",
    "train_df = pd.read_csv(\"combined_train.csv\", low_memory=False)\n",
    "train_df[\"label\"] = train_df[\"label\"].map({\"benign\": 0, \"malicious\": 1})\n",
    "train_df = train_df.loc[:, ~train_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "test_df = pd.read_csv(\"combined_test.csv\", low_memory=False)\n",
    "test_df[\"label\"] = test_df[\"label\"].map({\"benign\": 0, \"malicious\": 1})\n",
    "test_df = test_df.loc[:, ~test_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# ---- Top 10 feature selection ----\n",
    "correlations = train_df.corr(numeric_only=True)['label'].abs().sort_values(ascending=False)\n",
    "top_features = correlations.drop(\"label\").head(10).index.tolist()\n",
    "print(\"Top 10 correlated features:\", top_features)\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_df[top_features]\n",
    "t_train = train_df[\"label\"]\n",
    "X_test = test_df[top_features]\n",
    "t_test = test_df[\"label\"]\n",
    "\n",
    "# Impute missing values\n",
    "print(\"Handling missing values...\")\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Feature scaling\n",
    "print(\"Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    # \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1),\n",
    "    \"SVM\": SVC(),\n",
    "    \"MLP\": MLPClassifier(max_iter=500)\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "print(\"Training and evaluating models...\")\n",
    "for name in tqdm(models):\n",
    "    model = models[name]\n",
    "    model.fit(X_train, t_train)\n",
    "    preds = model.predict(X_test)\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(classification_report(t_test, preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMVBSJCVTZvC"
   },
   "outputs": [],
   "source": [
    "# Definitions of Buffer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ysuxp9swTdto"
   },
   "outputs": [],
   "source": [
    "print(t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prHuLzekX0JX"
   },
   "outputs": [],
   "source": [
    "# Definitions of classes for Trustrank calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4w9lfvKwTib9"
   },
   "outputs": [],
   "source": [
    "# Hyper parameter pipeline for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9B1L0_7rUoli"
   },
   "outputs": [],
   "source": [
    "# Actual Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-xJWaM4XxLh"
   },
   "outputs": [],
   "source": [
    "# Trustrank results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXItL03WUaXa"
   },
   "outputs": [],
   "source": [
    "# Graphs creation/Plots/Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qr1lWCLgUeWK"
   },
   "outputs": [],
   "source": [
    "# Code for model saving, creation of non matplotlib graphs, and gifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0umABl-6U0Ie"
   },
   "outputs": [],
   "source": [
    "# Display graphs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
